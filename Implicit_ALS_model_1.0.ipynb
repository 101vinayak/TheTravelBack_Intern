{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\ndf = pd.read_csv(\"new_sample_data.csv\", usecols = [\"Unnamed: 0\",\"user_id\", \"restaurant_id\", \"Rating_x\", \"title\", \"clicked\"])\\ndf.columns\\nuser_list = []\\nuser_list = list(df[\"user_id\"].unique())\\nres_list = []\\nres_list = list(df[df[\"user_id\"] == 0][[\"title\", \"clicked\"]][\"title\"].unique())\\nlength = len(res_list)\\nusers_list = []\\nrest_list = []\\nrest_name = []\\nfor j in range(len(user_list)):\\n    res_list = []\\n    res_list = list(df[df[\"user_id\"] == user_list[j]][[\"title\", \"clicked\"]][\"title\"].unique())\\n    for i in range(len(res_list)):\\n        resid = list(df[df[\"title\"] == res_list[i]][\"restaurant_id\"].unique())\\n        rest_list.append(resid)\\n        rest_name.append(res_list[i])\\n        users_list.append(user_list[j])\\nfor i in range(len(rest_list)):\\n    s = str(rest_list[i][0])\\n    num = int(s)\\n    rest_list[i] = num\\nclicked = []\\nclicked = [1]*len(rest_list)\\nimport csv\\nwith open(\"implicit_restaurants.csv\", \"w\", newline = \"\") as f:\\n    writer = csv.writer(f)\\n    l = [[\"user_id\", \"restaurant_id\", \"title\", \"event_clicked\"]]\\n    writer.writerows(l)\\n    ll = []\\n    for i in range(len(rest_list)):\\n        ll.append([users_list[i], rest_list[i], rest_name[i], clicked[i]])\\n    writer.writerows(ll)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"new_sample_data.csv\", usecols = [\"Unnamed: 0\",\"user_id\", \"restaurant_id\", \"Rating_x\", \"title\", \"clicked\"])\n",
    "df.columns\n",
    "user_list = []\n",
    "user_list = list(df[\"user_id\"].unique())\n",
    "res_list = []\n",
    "res_list = list(df[df[\"user_id\"] == 0][[\"title\", \"clicked\"]][\"title\"].unique())\n",
    "length = len(res_list)\n",
    "users_list = []\n",
    "rest_list = []\n",
    "rest_name = []\n",
    "for j in range(len(user_list)):\n",
    "    res_list = []\n",
    "    res_list = list(df[df[\"user_id\"] == user_list[j]][[\"title\", \"clicked\"]][\"title\"].unique())\n",
    "    for i in range(len(res_list)):\n",
    "        resid = list(df[df[\"title\"] == res_list[i]][\"restaurant_id\"].unique())\n",
    "        rest_list.append(resid)\n",
    "        rest_name.append(res_list[i])\n",
    "        users_list.append(user_list[j])\n",
    "for i in range(len(rest_list)):\n",
    "    s = str(rest_list[i][0])\n",
    "    num = int(s)\n",
    "    rest_list[i] = num\n",
    "clicked = []\n",
    "clicked = [1]*len(rest_list)\n",
    "import csv\n",
    "with open(\"implicit_restaurants.csv\", \"w\", newline = \"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    l = [[\"user_id\", \"restaurant_id\", \"title\", \"event_clicked\"]]\n",
    "    writer.writerows(l)\n",
    "    ll = []\n",
    "    for i in range(len(rest_list)):\n",
    "        ll.append([users_list[i], rest_list[i], rest_name[i], clicked[i]])\n",
    "    writer.writerows(ll)\n",
    "\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'implicit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f11fde11264f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mimplicit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'implicit'"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "import random\n",
    "import implicit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting implicit\n",
      "  Using cached implicit-0.4.4.tar.gz (1.1 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from implicit) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from implicit) (1.5.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from implicit) (4.47.0)\n",
      "Building wheels for collected packages: implicit\n",
      "  Building wheel for implicit (setup.py): started\n",
      "  Building wheel for implicit (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for implicit\n",
      "Failed to build implicit\n",
      "Installing collected packages: implicit\n",
      "    Running setup.py install for implicit: started\n",
      "    Running setup.py install for implicit: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'c:\\users\\lenovo\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-lk8qgle8\\\\implicit\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-lk8qgle8\\\\implicit\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-wheel-y7b30fey'\n",
      "       cwd: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-lk8qgle8\\implicit\\\n",
      "  Complete output (26 lines):\n",
      "  WARNING:root:The nvcc binary could not be located in your $PATH. Either add it to your path, or set $CUDAHOME to enable CUDA extensions\n",
      "  Failed to find CUDA toolkit. Building without GPU acceleration.\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\implicit\n",
      "  copying implicit\\als.py -> build\\lib.win-amd64-3.8\\implicit\n",
      "  copying implicit\\approximate_als.py -> build\\lib.win-amd64-3.8\\implicit\n",
      "  copying implicit\\nearest_neighbours.py -> build\\lib.win-amd64-3.8\\implicit\n",
      "  copying implicit\\utils.py -> build\\lib.win-amd64-3.8\\implicit\n",
      "  copying implicit\\__init__.py -> build\\lib.win-amd64-3.8\\implicit\n",
      "  creating build\\lib.win-amd64-3.8\\implicit\\cuda\n",
      "  copying implicit\\cuda\\__init__.py -> build\\lib.win-amd64-3.8\\implicit\\cuda\n",
      "  creating build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "  copying implicit\\datasets\\lastfm.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "  copying implicit\\datasets\\million_song_dataset.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "  copying implicit\\datasets\\movielens.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "  copying implicit\\datasets\\reddit.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "  copying implicit\\datasets\\sketchfab.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "  copying implicit\\datasets\\_download.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "  copying implicit\\datasets\\__init__.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "  running build_ext\n",
      "  building 'implicit._als' extension\n",
      "  error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for implicit\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'c:\\users\\lenovo\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-lk8qgle8\\\\implicit\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-lk8qgle8\\\\implicit\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-record-a8fdl9w_\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\lenovo\\anaconda3\\Include\\implicit'\n",
      "         cwd: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-lk8qgle8\\implicit\\\n",
      "    Complete output (26 lines):\n",
      "    WARNING:root:The nvcc binary could not be located in your $PATH. Either add it to your path, or set $CUDAHOME to enable CUDA extensions\n",
      "    Failed to find CUDA toolkit. Building without GPU acceleration.\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.8\n",
      "    creating build\\lib.win-amd64-3.8\\implicit\n",
      "    copying implicit\\als.py -> build\\lib.win-amd64-3.8\\implicit\n",
      "    copying implicit\\approximate_als.py -> build\\lib.win-amd64-3.8\\implicit\n",
      "    copying implicit\\nearest_neighbours.py -> build\\lib.win-amd64-3.8\\implicit\n",
      "    copying implicit\\utils.py -> build\\lib.win-amd64-3.8\\implicit\n",
      "    copying implicit\\__init__.py -> build\\lib.win-amd64-3.8\\implicit\n",
      "    creating build\\lib.win-amd64-3.8\\implicit\\cuda\n",
      "    copying implicit\\cuda\\__init__.py -> build\\lib.win-amd64-3.8\\implicit\\cuda\n",
      "    creating build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "    copying implicit\\datasets\\lastfm.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "    copying implicit\\datasets\\million_song_dataset.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "    copying implicit\\datasets\\movielens.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "    copying implicit\\datasets\\reddit.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "    copying implicit\\datasets\\sketchfab.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "    copying implicit\\datasets\\_download.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "    copying implicit\\datasets\\__init__.py -> build\\lib.win-amd64-3.8\\implicit\\datasets\n",
      "    running build_ext\n",
      "    building 'implicit._als' extension\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'c:\\users\\lenovo\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-lk8qgle8\\\\implicit\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-lk8qgle8\\\\implicit\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-record-a8fdl9w_\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\lenovo\\anaconda3\\Include\\implicit' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"implicit_restaurants.csv\")\n",
    "df_1 = df_1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df_1.groupby(['user_id', 'restaurant_id', 'title']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>restaurant_id</th>\n",
       "      <th>title</th>\n",
       "      <th>event_clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "      <td>A&amp;W Restaurant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>363</td>\n",
       "      <td>Chartreuse Moose Cappuccino Bar &amp; Bistro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>Jake's</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>373</td>\n",
       "      <td>P Bass Fish and Chips</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>204</td>\n",
       "      <td>Canadian 2 For 1 Pizza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>19</td>\n",
       "      <td>204</td>\n",
       "      <td>Canadian 2 For 1 Pizza</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>19</td>\n",
       "      <td>217</td>\n",
       "      <td>A&amp;W Restaurant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>19</td>\n",
       "      <td>370</td>\n",
       "      <td>Jake's</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>19</td>\n",
       "      <td>373</td>\n",
       "      <td>P Bass Fish and Chips</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>19</td>\n",
       "      <td>380</td>\n",
       "      <td>Smitty's 100 Mile House</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  restaurant_id                                     title  \\\n",
       "0         0            217                            A&W Restaurant   \n",
       "1         0            363  Chartreuse Moose Cappuccino Bar & Bistro   \n",
       "2         0            370                                    Jake's   \n",
       "3         0            373                     P Bass Fish and Chips   \n",
       "4         1            204                    Canadian 2 For 1 Pizza   \n",
       "..      ...            ...                                       ...   \n",
       "87       19            204                    Canadian 2 For 1 Pizza   \n",
       "88       19            217                            A&W Restaurant   \n",
       "89       19            370                                    Jake's   \n",
       "90       19            373                     P Bass Fish and Chips   \n",
       "91       19            380                   Smitty's 100 Mile House   \n",
       "\n",
       "    event_clicked  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "..            ...  \n",
       "87              1  \n",
       "88              1  \n",
       "89              1  \n",
       "90              1  \n",
       "91              1  \n",
       "\n",
       "[92 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d08c6eb0f1c4961b3123d0290fa4b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df['title'] = grouped_df['title'].astype(\"category\")\n",
    "grouped_df['user_id'] = grouped_df['user_id'].astype(\"category\")\n",
    "grouped_df['restaurant_id'] = grouped_df['restaurant_id'].astype(\"category\")\n",
    "grouped_df['user_id'] = grouped_df['user_id'].cat.codes\n",
    "grouped_df['restaurant_id'] = grouped_df['restaurant_id'].cat.codes\n",
    "\n",
    "sparse_content_person = sparse.csr_matrix((grouped_df['event_clicked'].astype(float), (grouped_df['restaurant_id'], grouped_df['user_id'])))\n",
    "sparse_person_content = sparse.csr_matrix((grouped_df['event_clicked'].astype(float), (grouped_df['user_id'], grouped_df['restaurant_id'])))\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=50)\n",
    "\n",
    "alpha = 15\n",
    "data = (sparse_content_person * alpha).astype('double')\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A&W Restaurant\n",
      "El Caballo Restaurant\n",
      "Jake's\n",
      "Subway\n",
      "Chartreuse Moose Cappuccino Bar & Bistro\n",
      "P Bass Fish and Chips\n",
      "Smitty's 100 Mile House\n",
      "Tim Hortons\n",
      "Farrier pub\n",
      "The Great Wok\n"
     ]
    }
   ],
   "source": [
    "content_id = 3\n",
    "n_similar = 10\n",
    "\n",
    "person_vecs = model.user_factors\n",
    "content_vecs = model.item_factors\n",
    "\n",
    "content_norms = np.sqrt((content_vecs * content_vecs).sum(axis=1))\n",
    "scores = content_vecs.dot(content_vecs[content_id]) / content_norms\n",
    "top_idx = np.argpartition(scores, -n_similar)[-n_similar:]\n",
    "similar = sorted(zip(top_idx, scores[top_idx] / content_norms[content_id]), key=lambda x: -x[1])\n",
    "for content in similar:\n",
    "    idx, score = content\n",
    "    print(grouped_df.title.loc[grouped_df.restaurant_id == idx].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14    Tim Hortons\n",
       "54    Tim Hortons\n",
       "82    Tim Hortons\n",
       "Name: title, dtype: category\n",
       "Categories (15, object): ['A&W Restaurant', 'BJ's Donuts & Eatery', 'Blue Sky Chinese Restaurant', 'Canadian 2 For 1 Pizza', ..., 'Subway', 'Sushi and Noodle', 'The Great Wok', 'Tim Hortons']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df[grouped_df[\"restaurant_id\"] == 1][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(person_id, sparse_person_content, person_vecs, content_vecs, num_contents=10):\n",
    "    # Get the interactions scores from the sparse person content matrix\n",
    "    person_interactions = sparse_person_content[person_id,:].toarray()\n",
    "    # Add 1 to everything, so that articles with no interaction yet become equal to 1\n",
    "    person_interactions = person_interactions.reshape(-1) + 1\n",
    "    # Make articles already interacted zero\n",
    "    person_interactions[person_interactions > 1] = 0\n",
    "    # Get dot product of person vector and all content vectors\n",
    "    rec_vector = person_vecs[person_id,:].dot(content_vecs.T).toarray()\n",
    "    \n",
    "    # Scale this recommendation vector between 0 and 1\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "    # Content already interacted have their recommendation multiplied by zero\n",
    "    recommend_vector = person_interactions * rec_vector_scaled\n",
    "    # Sort the indices of the content into order of best recommendations\n",
    "    content_idx = np.argsort(recommend_vector)[::-1][:num_contents]\n",
    "    \n",
    "    # Start empty list to store titles and scores\n",
    "    titles = []\n",
    "    scores = []\n",
    "\n",
    "    for idx in content_idx:\n",
    "        # Append titles and scores to the list\n",
    "        titles.append(grouped_df.title.loc[grouped_df.restaurant_id == idx].iloc[0])\n",
    "        scores.append(recommend_vector[idx])\n",
    "\n",
    "    recommendations = pd.DataFrame({'title': titles, 'score': scores})\n",
    "\n",
    "    return recommendations\n",
    "    \n",
    "# Get the trained person and content vectors. We convert them to csr matrices\n",
    "person_vecs = sparse.csr_matrix(model.user_factors)\n",
    "content_vecs = sparse.csr_matrix(model.item_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because you visited A&W Restaurant\n",
      "                         title     score\n",
      "0                       Subway  0.024795\n",
      "1                  Tim Hortons  0.022401\n",
      "2                The Great Wok  0.018646\n",
      "3       Canadian 2 For 1 Pizza  0.017721\n",
      "4                  Farrier pub  0.016547\n",
      "5         BJ's Donuts & Eatery  0.016525\n",
      "6             Sushi and Noodle  0.014690\n",
      "7  Blue Sky Chinese Restaurant  0.007333\n",
      "8      Smitty's 100 Mile House  0.005884\n",
      "9        El Caballo Restaurant  0.003575\n"
     ]
    }
   ],
   "source": [
    "person_id = 0\n",
    "recommendations = recommend(person_id, sparse_person_content, person_vecs, content_vecs)\n",
    "print(\"Because you visited\", grouped_df[grouped_df[\"user_id\"] == person_id][\"title\"][0])\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_train(ratings, pct_test = 0.2):\n",
    "    test_set = ratings.copy() # Make a copy of the original set to be the test set. \n",
    "    test_set[test_set != 0] = 1 # Store the test set as a binary preference matrix\n",
    "    \n",
    "    training_set = ratings.copy() # Make a copy of the original data we can alter as our training set. \n",
    "    \n",
    "    nonzero_inds = training_set.nonzero() # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) # Zip these pairs together of item,user index into list\n",
    "\n",
    "    \n",
    "    random.seed(0) # Set the random seed to zero for reproducibility\n",
    "    \n",
    "    num_samples = int(np.ceil(pct_test*len(nonzero_pairs))) # Round the number of samples needed to the nearest integer\n",
    "    samples = random.sample(nonzero_pairs, num_samples) # Sample a random number of item-user pairs without replacement\n",
    "\n",
    "    content_inds = [index[0] for index in samples] # Get the item row indices\n",
    "\n",
    "    person_inds = [index[1] for index in samples] # Get the user column indices\n",
    "\n",
    "    \n",
    "    training_set[content_inds, person_inds] = 0 # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set.eliminate_zeros() # Get rid of zeros in sparse array storage after update to save space\n",
    "    \n",
    "    return training_set, test_set, list(set(person_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_train, content_test, content_persons_altered = make_train(sparse_content_person, pct_test = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(predictions, test):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_auc(training_set, altered_persons, predictions, test_set):\n",
    "    store_auc = [] # An empty list to store the AUC for each user that had an item removed from the training set\n",
    "    popularity_auc = [] # To store popular AUC scores\n",
    "    pop_contents = np.array(test_set.sum(axis = 1)).reshape(-1) # Get sum of item iteractions to find most popular\n",
    "    content_vecs = predictions[1]\n",
    "    for person in altered_persons: # Iterate through each user that had an item altered\n",
    "        training_column = training_set[:,person].toarray().reshape(-1) # Get the training set column\n",
    "        zero_inds = np.where(training_column == 0) # Find where the interaction had not yet occurred\n",
    "        \n",
    "        # Get the predicted values based on our user/item vectors\n",
    "        person_vec = predictions[0][person,:]\n",
    "        pred = person_vec.dot(content_vecs).toarray()[0,zero_inds].reshape(-1)\n",
    "        \n",
    "        # Get only the items that were originally zero\n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = test_set[:,person].toarray()[zero_inds,0].reshape(-1)\n",
    "        \n",
    "        # Select the binarized yes/no interaction pairs from the original full data\n",
    "        # that align with the same pairs in training \n",
    "        pop = pop_contents[zero_inds] # Get the item popularity for our chosen items\n",
    "        \n",
    "        store_auc.append(auc_score(pred, actual)) # Calculate AUC for the given user and store\n",
    "        \n",
    "        popularity_auc.append(auc_score(pop, actual)) # Calculate AUC using most popular and score\n",
    "    # End users iteration\n",
    "    \n",
    "    return float('%.3f'%np.mean(store_auc)), float('%.3f'%np.mean(popularity_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.734)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_mean_auc(content_train, content_persons_altered,\n",
    "              [person_vecs, content_vecs.T], content_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
