{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for all outdoor activity and ice skating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait         # driver to control chrome browser\n",
    "import pyautogui                            \n",
    "from bs4 import BeautifulSoup as bs                               # to parse the html code\n",
    "import threading                                                  # to do multi threding\n",
    "from lxml import html\n",
    "import time \n",
    "import pandas as pd   \n",
    "import csv  \n",
    "\n",
    "def get_driver():\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument(\"--disable-logging\")\n",
    "    options.add_argument('log-level=3')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "\n",
    "    return webdriver.Chrome(executable_path=r'chromedriver.exe', options=options)\n",
    "\n",
    "def write_output(filename, data): # create new csv\n",
    "    with open(filename, mode=\"w\", newline='',encoding='UTF-8') as out_file:\n",
    "        writer = csv.writer(out_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "        writer.writerow(['Name','Rating','No Review','Address','Timing'])\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "driver = get_driver()\n",
    "\n",
    "def process(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    while(True):\n",
    "        next_btn = driver.find_element_by_xpath('//*[@id=\"n7lv7yjyC35__section-pagination-button-next\"]')\n",
    "        try:\n",
    "            for i in range(1,21):\n",
    "                p = \"ZXf8if5__or:{0}\".format(i).replace('\"\"',\"''\")\n",
    "                st = \"//div[@data-section-id='{0}']\".format(p)\n",
    "                each_review = driver.find_element_by_xpath(str(st))\n",
    "                each_review.click()\n",
    "                time.sleep(3)\n",
    "\n",
    "                soup = bs(driver.page_source, \"lxml\")\n",
    "\n",
    "                try:\n",
    "                    name = soup.find(\"h1\",{\"class\":\"section-hero-header-title-title GLOBAL__gm2-headline-5\"}).find(\"span\").text\n",
    "                except:\n",
    "                    name = \"N/A\"\n",
    "                try:\n",
    "                    no_review = soup.find(\"button\",{\"class\":\"widget-pane-link\"}).text.replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\")\n",
    "                except:\n",
    "                    no_review = \"N/A\"\n",
    "                try:\n",
    "                    rating = soup.find(\"span\",{\"class\":\"section-star-display\"}).text\n",
    "                except:\n",
    "                    rating = \"N/A\"\n",
    "                try:\n",
    "                    address = soup.find(\"div\", {\"class\": \"ugiz4pqJLAG__primary-text gm2-body-2\"}).text\n",
    "                except:\n",
    "                    address = \"N/A\"\n",
    "                \n",
    "                timing = {}\n",
    "                try:\n",
    "                    btn = driver.find_element_by_xpath('//span[@class=\"cX2WmPgCkHi__section-open-hours-button-right section-open-hours-button cX2WmPgCkHi__expand-more\"]').click() \n",
    "                    soup1 = bs(driver.page_source, \"lxml\")\n",
    "                    all_time = soup1.find_all(\"tr\", {\"class\": \"lo7U087hsMA__row-row\"})\n",
    "                    \n",
    "                    for i in all_time:\n",
    "                        timing[i.text.split('   ')[0].replace(' ', '')] = i.text.split('   ')[1]\n",
    "                except:\n",
    "                    timing[\"time\"] = \"Not given\"\n",
    "\n",
    "                info = []\n",
    "                info.append(name)\n",
    "                info.append(rating)\n",
    "                info.append(no_review)\n",
    "                info.append(address)\n",
    "                info.append(timing)\n",
    "                yield(info)\n",
    "\n",
    "                print(info)\n",
    "                back = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/button').click()\n",
    "                time.sleep(2)\n",
    "\n",
    "            next_btn.click()\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            next_btn = driver.find_element_by_xpath('//*[@id=\"n7lv7yjyC35__section-pagination-button-next\"]').click()\n",
    "            time.sleep(3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = [\"https://www.google.com/maps/search/outdoor+activities+in+british+columbia/@50.6131926,-126.920475,4z/data=!3m1!4b1\", \"https://www.google.com/maps/search/ice+skating+in+british+columbia/@52.0579407,-126.344865,6z/data=!3m1!4b1\"]\n",
    "    filename = [\"data1.csv\", \"data2.csv\"]\n",
    "    for i, filename in zip(url,filename):\n",
    "        data = process(i)\n",
    "        write_output(filename, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping all reviews of each and every listed shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import re as r\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "#this part of the code below chooses a header at random from a collection of 10 headers\n",
    "header_list=[{'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36COOKIE: 5=2; ax=v167-7'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/74.0.3729.157 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/79.0.3945.88 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Datanyze; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.24 Safari/537.36'}]\n",
    "\n",
    "\n",
    "def head():\n",
    "    n = random.random()\n",
    "    n=int(n*7)\n",
    "    return header_list[n]\n",
    "\n",
    "def scrape(address):\n",
    "    path_chrome = r\"chromedriver\"\n",
    "    ua = head()\n",
    "    user_agent = \"user-agent=\"+ua['User-Agent']\n",
    "    chrome_options= webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(user_agent)\n",
    "    print('user-agent\\n',user_agent)\n",
    "    driver = webdriver.Chrome(path_chrome,options=chrome_options)\n",
    "    print('opening defualt page')\n",
    "    url  = 'https://www.google.com/maps/@30.7678096,76.4684188,14z'\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    element = driver.find_element_by_xpath(\"//input[@id='searchboxinput']\")\n",
    "    search  = driver.find_element_by_xpath(\"//button[@id='searchbox-searchbutton']\")\n",
    "    element.clear()\n",
    "    element.send_keys(address)\n",
    "    search.click()\n",
    "\n",
    "    time.sleep(15)\n",
    "    button_status = 0\n",
    "    for status in range(1,6):\n",
    "        print('trying to press SEE ALL REVIEWS button, attempt {}'.format(status))\n",
    "        if(button_status !=1):\n",
    "            try:\n",
    "                wait = WebDriverWait(driver, 15)\n",
    "                wait.until(EC.element_to_be_clickable(\n",
    "                (By.XPATH, \"//button[@class='allxGeDnJMl__button allxGeDnJMl__button-text']\"))).click()\n",
    "                print('button pressed: see all reviews ')\n",
    "                button_status = 1\n",
    "            except:\n",
    "                continue\n",
    "                print('button not found')\n",
    "        if(button_status==1):\n",
    "            break\n",
    "        time.sleep(5)\n",
    "\n",
    "    time.sleep(1)\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    com_ = response.find_all('div', class_ = 'jqnFjrOWMVU__root')[0].text\n",
    "    com = com_.split(' ')\n",
    "    combined_ratings = com[47]\n",
    "    try:\n",
    "        total_reviews = int(com[49].replace(',',''))\n",
    "    except:\n",
    "        total_reviews = int(com[50].replace(',',''))\n",
    "    tags  = response.find_all('span', class_ = 'tuPVDR7ouq5__text gm2-body-2')\n",
    "    count = response.find_all('span', class_ = 'tuPVDR7ouq5__subtext gm2-caption')\n",
    "    tags_filtered = tags[1:-1]\n",
    "    tags_list = []\n",
    "    for i in range(len(tags_filtered)):\n",
    "        tags_list.append(tags_filtered[i].text)\n",
    "    count_list=[]\n",
    "    for i in count:\n",
    "        count_list.append(i.text)\n",
    "    tags_dict = dict(zip(tags_list,count_list))\n",
    "\n",
    "    print(combined_ratings)\n",
    "    print(total_reviews)\n",
    "\n",
    "    fname = \"data.json\"\n",
    "    if os.path.isfile(fname):\n",
    "        # File exists\n",
    "        with open(fname, 'a+') as outfile:\n",
    "            outfile.write(',')\n",
    "            tags  = response.find_all('span', class_ = 'tuPVDR7ouq5__text gm2-body-2')\n",
    "            count = response.find_all('span', class_ = 'tuPVDR7ouq5__subtext gm2-caption')\n",
    "            tags_filtered = tags[1:-1]\n",
    "            tags_list = []\n",
    "            for i in range(len(tags_filtered)):\n",
    "                tags_list.append(tags_filtered[i].text)\n",
    "            count_list=[]\n",
    "            for i in count:\n",
    "                count_list.append(i.text)\n",
    "            tags_dict = dict(zip(tags_list,count_list))\n",
    "            \n",
    "            ll = str(driver.current_url)\n",
    "            i = ll.find('@')\n",
    "            url = ll[i+1:].split(',')\n",
    "            \n",
    "            lattitude = url[0]\n",
    "            longitude = url[1]\n",
    "            \n",
    "            data = {}\n",
    "            data['address'] = address\n",
    "            data['combined_ratings'] = combined_ratings\n",
    "            data['total_reviews'] = total_reviews\n",
    "            data['tags_dict'] = tags_dict\n",
    "            data['lattitude'] = lattitude\n",
    "            data['longitude'] = longitude\n",
    "            \n",
    "            json.dump(data, outfile)\n",
    "\n",
    "    print('got tags and combined reviews. You can check the directory')\n",
    "    print('Pressing scroll button')\n",
    "    #int(total_reviews/12)  for final code\n",
    "    #x = int(total_reviews/35) \n",
    "    x = 10\n",
    "    for i in range(10):\n",
    "        print('progress {}/{}'.format(i,x))\n",
    "        try:\n",
    "            elem = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"[class='section-loading-spinner']\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\",elem)\n",
    "        except Exception:\n",
    "            break\n",
    "        try:\n",
    "            for see_more in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"button[class^='section-expand-review']\"))):\n",
    "                see_more.click()\n",
    "        except:\n",
    "            continue\n",
    "    time.sleep(2)\n",
    "    path = r'C:\\Users\\RMohit\\Downloads\\The Travel Back\\Web Scraping\\Task 6 - google map scraping\\temp_storage'\n",
    "    filename = os.path.join(path,address+\"_reviews.csv\")\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, \"w\", encoding='utf8',newline=\"\\n\") as f:\n",
    "            csvwriter=csv.writer(f)\n",
    "            csvwriter.writerow([\"name\",\"rating\",\"review\",\"time\"])    \n",
    "            response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            name_    = response.find_all('div', class_='section-review-title')\n",
    "            ratings_ = response.find_all('span', class_='section-review-stars')\n",
    "            reviews_ = response.find_all('div', class_ = 'section-review-review-content')\n",
    "            time_    = response.find_all('div', class_ = 'section-review-metadata section-review-metadata-with-note')\n",
    "\n",
    "            for i in range(len(name_)):\n",
    "                nam = name_[i].text\n",
    "                rat = ratings_[i]\n",
    "                rat = rat['aria-label']\n",
    "                rav = reviews_[i].text\n",
    "                tim = time_[i].text\n",
    "                csvwriter.writerow([nam,rat,rav,tim])\n",
    "    print('execution complete, closing the chrome')\n",
    "    time.sleep(5)\n",
    "    chrome_options.arguments.remove(user_agent)\n",
    "    driver.close()\n",
    "\n",
    "filename = [\"data1.csv\",\"data2.csv\"]\n",
    "for i in filename:\n",
    "    restaurant_data = pd.read_csv(i)\n",
    "    restaurant_data['keyword'] = restaurant_data['Name'] +\" \" + restaurant_data['Address']\n",
    "    keywords_list = list(restaurant_data['keyword'])\n",
    "    l = len(keywords_list)\n",
    "    print(l)\n",
    "    for i in range(0,l):\n",
    "        key = keywords_list[i]\n",
    "        print(key)\n",
    "        try:\n",
    "            scrape(key)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## google map image scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import re as r\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "import cssutils\n",
    "\n",
    "#this part of the code below chooses a header at random from a collection of 10 headers\n",
    "header_list=[{'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36COOKIE: 5=2; ax=v167-7'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/74.0.3729.157 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/79.0.3945.88 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Datanyze; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.24 Safari/537.36'}]\n",
    "\n",
    "\n",
    "def head():\n",
    "    n = random.random()\n",
    "    n=int(n*7)\n",
    "    return header_list[n]\n",
    "\n",
    "def scrape(address):\n",
    "    path_chrome = r\"chromedriver\"\n",
    "    ua = head()\n",
    "    user_agent = \"user-agent=\"+ua['User-Agent']\n",
    "    chrome_options= webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(user_agent)\n",
    "    print('user-agent\\n',user_agent)\n",
    "    driver = webdriver.Chrome(path_chrome,options=chrome_options)\n",
    "    print('opening defualt page')\n",
    "    url  = 'https://www.google.com/maps/@30.7678096,76.4684188,14z'\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    element = driver.find_element_by_xpath(\"//input[@id='searchboxinput']\")\n",
    "    search  = driver.find_element_by_xpath(\"//button[@id='searchbox-searchbutton']\")\n",
    "    element.clear()\n",
    "    element.send_keys(address)\n",
    "    search.click()\n",
    "\n",
    "    time.sleep(15)\n",
    "    \n",
    "    all_img = driver.find_element_by_xpath(\"//div[@class='section-carouselphoto-photo-container-shim']\").click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    \n",
    "    #images\n",
    "    results = soup.findAll('div', {'class': 'gallery-image-low-res'})\n",
    "    print(len(results))\n",
    "    j=0\n",
    "    if(len(results) > 0):\n",
    "        os.mkdir(\"images/{0}\".format(address))\n",
    "    for i in results:\n",
    "        i = \"\"\"{0}\"\"\".format(i)\n",
    "        soup1 = BeautifulSoup(i,'html.parser')\n",
    "        div_style = soup1.find('div')['style']\n",
    "        style = cssutils.parseStyle(div_style)\n",
    "        url = style['background-image'].replace('url(', '').replace(')', '')\n",
    "        try:\n",
    "            index = url.find('=')\n",
    "            url = url[:index] + \"=w1080-h1080-k-no\"\n",
    "        except:\n",
    "            continue\n",
    "        #print(url)\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(\"images\", address , \"image\"+str(j)+'.jpg'))\n",
    "        except:\n",
    "            continue\n",
    "        j += 1\n",
    "\n",
    "    driver.close()\n",
    "    \n",
    "restaurant_data = pd.read_csv('data1.csv')\n",
    "restaurant_data['keyword'] = restaurant_data['Name'] +\" \" + restaurant_data['Address']\n",
    "keywords_list = list(restaurant_data['keyword'])\n",
    "l = len(keywords_list)\n",
    "print(l)\n",
    "for i in range(0,l):\n",
    "    key = keywords_list[i]\n",
    "    print(key)\n",
    "    try:\n",
    "        scrape(key)\n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
