{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import random\n",
    "from multiprocessing.pool import ThreadPool, Pool\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import re as r\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "import urllib\n",
    "import cssutils\n",
    "\n",
    "#this part of the code below chooses a header at random from a collection of 10 headers\n",
    "header_list=[{'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36COOKIE: 5=2; ax=v167-7'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/74.0.3729.157 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/79.0.3945.88 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Datanyze; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'},\n",
    "       {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.24 Safari/537.36'}]\n",
    "\n",
    "\n",
    "def head():\n",
    "    n = random.random()\n",
    "    n=int(n*7)\n",
    "    return header_list[n]\n",
    "\n",
    "def write_output(data):\n",
    "    if(not os.path.isfile('link.csv')):\n",
    "        with open(\"link.csv\", mode=\"w\", newline='',encoding='UTF-8') as out_file:\n",
    "            writer = csv.writer(out_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "            writer.writerow(['link'])\n",
    "        \n",
    "    with open(\"link.csv\", mode=\"a\", newline='',encoding='UTF-8') as out_file:\n",
    "        writer = csv.writer(out_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "        writer.writerow(data)\n",
    "\n",
    "\n",
    "def scrape(address):\n",
    "    path_chrome = r\"chromedriver\"\n",
    "    ua = head()\n",
    "    user_agent = \"user-agent=\"+ua['User-Agent']\n",
    "    chrome_options= webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(user_agent)\n",
    "    print('user-agent\\n',user_agent)\n",
    "    driver = webdriver.Chrome(path_chrome,options=chrome_options)\n",
    "    print('opening defualt page')\n",
    "    url  = 'https://www.google.com/maps/@30.7678096,76.4684188,14z'\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    element = driver.find_element_by_xpath(\"//input[@id='searchboxinput']\")\n",
    "    search  = driver.find_element_by_xpath(\"//button[@id='searchbox-searchbutton']\")\n",
    "    element.clear()\n",
    "    element.send_keys(address)\n",
    "    search.click()\n",
    "\n",
    "    time.sleep(15)\n",
    "    \n",
    "    all_img = driver.find_element_by_xpath(\"//div[@class='section-carouselphoto-photo-container-shim']\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    \n",
    "    #images\n",
    "    results = soup.findAll('div', {'class': 'gallery-image-low-res'})\n",
    "    print(len(results))\n",
    "    \n",
    "    link = [[],[]]\n",
    "    for i in results:\n",
    "        i = \"\"\"{0}\"\"\".format(i)\n",
    "        soup1 = BeautifulSoup(i,'html.parser')\n",
    "        div_style = soup1.find('div')['style']\n",
    "        style = cssutils.parseStyle(div_style)\n",
    "        url = style['background-image'].replace('url(', '').replace(')', '')\n",
    "        try:\n",
    "            index = url.find('=')\n",
    "            url = url[:index] + \"=w1080-h1080-k-no\"\n",
    "        except:\n",
    "            url = \"None\"\n",
    "        if(\"googleusercontent\" in url):\n",
    "            link[0].append(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        menu_btn = driver.find_element_by_xpath('//button[@class=\"spF7nQ9kj1t__tab\"]').click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "        #images\n",
    "        results = soup.find_all('div', {'class': 'gallery-image-low-res'})\n",
    "        print(len(results))\n",
    "        for i in results:\n",
    "            i = \"\"\"{0}\"\"\".format(i)\n",
    "            soup1 = BeautifulSoup(i,'html.parser')\n",
    "            div_style = soup1.find('div')['style']\n",
    "            style = cssutils.parseStyle(div_style)\n",
    "            url = style['background-image'].replace('url(', '').replace(')', '')\n",
    "            try:\n",
    "                index = url.find('=')\n",
    "                url = url[:index] + \"=w1080-h1080-k-no\"\n",
    "            except:\n",
    "                url = \"None\"\n",
    "            if(\"googleusercontent\" in url):\n",
    "                link[1].append(url)\n",
    "    except:\n",
    "        print(\"menu not found\")\n",
    "    time.sleep(2)\n",
    "    yield(link)\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    restaurant_data = pd.read_csv('Restaurants_Data_of_all_Cities_in_British_Columbia.csv')\n",
    "    restaurant_data['keyword'] = restaurant_data['Name'] +\" \" + restaurant_data['Address']\n",
    "    keywords_list = list(restaurant_data['keyword'])\n",
    "    l = len(keywords_list)\n",
    "    for i in keywords_list[0:l]:\n",
    "        try:\n",
    "            data = scrape(i)\n",
    "            write_output(data)\n",
    "        except:\n",
    "            print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
